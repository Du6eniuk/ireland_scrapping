{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is made to scrap Ireland information on scams\n",
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "html_response_check = requests.get('Deleted for security reasons')\n",
    "print(html_response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current script for scrapping ALL links, dates, and titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape links, dates, and titles from a given page\n",
    "def scrape_links(page_number):\n",
    "    url = f'Deleted for security reasons{page_number}'\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    articles = soup.find_all('div', class_='spotlight-content')\n",
    "    \n",
    "    link_details = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            date = article.find('span', class_='spotlight-label').get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            date = None\n",
    "        \n",
    "        try:\n",
    "            link = article.find('a', href=True)['href']\n",
    "        except TypeError:\n",
    "            link = None\n",
    "        \n",
    "        try:\n",
    "            title = article.find('h4', class_='spotlight-title').get_text(strip=True)\n",
    "        except AttributeError:\n",
    "            title = None\n",
    "        \n",
    "        if link:  # Only append if the link is found\n",
    "            link_details.append({'Date': date, 'Link': link, 'Title': title})\n",
    "    \n",
    "    return link_details\n",
    "\n",
    "# List to store all the links and their additional information\n",
    "all_links_details = []\n",
    "\n",
    "# Loop through each page and collect hrefs, dates, and titles\n",
    "for page in range(1, 3): # in range last number is excluding\n",
    "    print(f\"Scraping page {page}...\")\n",
    "    page_details = scrape_links(page)\n",
    "    all_links_details.extend(page_details)\n",
    "\n",
    "# Create a DataFrame from the collected details\n",
    "df = pd.DataFrame(all_links_details)\n",
    "\n",
    "today = date.today() # used to get todays date \n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel(f'scraped_links_details_{today}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current script for scrapping ALL new links inside cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "\n",
    "def scrape_website(link):\n",
    "    html_text = requests.get(link).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    \n",
    "    website = None\n",
    "    rows = soup.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) == 2:\n",
    "            key = cols[0].get_text(strip=True).replace(':', '')\n",
    "            value = cols[1].get_text(strip=True)\n",
    "            if key == 'Website':\n",
    "                website = value\n",
    "                break\n",
    "    \n",
    "    return website\n",
    "\n",
    "# Read the initial links from the 'scraped_links_details.xlsx' file\n",
    "df_links = pd.read_excel(f'scraped_links_details_{today}.xlsx')\n",
    "\n",
    "# List to store the website details\n",
    "website_details = []\n",
    "\n",
    "# Loop through each link and collect the website details\n",
    "for index, row in df_links.iterrows():\n",
    "    link = row['Link']\n",
    "    website = scrape_website(link)\n",
    "    website_details.append(website)\n",
    "    print(f\"Scraped website for {link}\")\n",
    "\n",
    "# Add the website details to the original DataFrame\n",
    "df_links['Website'] = website_details\n",
    "\n",
    "\n",
    "# Save the combined DataFrame to a new Excel file\n",
    "df_links.to_excel(f'website_details_{today}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current script for scrapping ALL old links inside cases. (old formatted cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unordinary_links(link):\n",
    "    html_text = requests.get(link).text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    \n",
    "    text = soup.get_text()\n",
    "    first_link = None\n",
    "    \n",
    "    # Regular expression to find the first URL in the text\n",
    "    match = re.search(r'http[s]?://\\S+', text)\n",
    "    if match:\n",
    "        first_link = match.group(0)\n",
    "    \n",
    "    return first_link\n",
    "\n",
    "# Read the initial links from the 'website_details.xlsx' file\n",
    "df_links = pd.read_excel('website_details.xlsx')\n",
    "\n",
    "# Loop through each link and update the \"Website\" column if it is empty\n",
    "for index, row in df_links.iterrows():\n",
    "    if pd.isna(row['Website']) or row['Website'] == '':\n",
    "        link = row['Link']\n",
    "        website = unordinary_links(link)\n",
    "        df_links.at[index, 'Website'] = website\n",
    "        print(f\"Scraped first link for {link}\")\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df_links.to_excel('updated_website_details.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
